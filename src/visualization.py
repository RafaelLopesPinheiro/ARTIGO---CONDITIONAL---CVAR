"""
M√≥dulo de visualiza√ß√£o para o sistema de aloca√ß√£o √≥tima.
Inclui compara√ß√£o de m√∫ltiplos modelos.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List

logger = logging.getLogger(__name__)

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10


def plot_model_comparison(comparison_df:pd.DataFrame, output_dir:Path):
    """
    Create comprehensive model comparison visualizations.
    
    Args:
        comparison_df:DataFrame with model comparison results
        output_dir:Directory to save figures
    """
    logger.info("Generating model comparison visualizations...")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Create main comparison figure
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # Color palette
    n_models = len(comparison_df)
    colors = sns.color_palette("husl", n_models)
    model_colors = dict(zip(comparison_df['model'], colors))
    
    # ========================================================================
    # 1.MAE Comparison (Bar)
    # ========================================================================
    ax1 = fig.add_subplot(gs[0, 0])
    comparison_df_sorted = comparison_df.sort_values('mae')
    bars = ax1.barh(comparison_df_sorted['model'], comparison_df_sorted['mae'], 
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax1.set_xlabel('MAE (Mean Absolute Error)', fontweight='bold')
    ax1.set_title('MAE Comparison\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax1.grid(axis='x', alpha=0.3)
    
    # Add values on bars
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax1.text(row['mae'] + 0.1, i, f"{row['mae']:.2f}", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 2.RMSE Comparison (Bar)
    # ========================================================================
    ax2 = fig.add_subplot(gs[0, 1])
    comparison_df_sorted = comparison_df.sort_values('rmse')
    bars = ax2.barh(comparison_df_sorted['model'], comparison_df_sorted['rmse'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax2.set_xlabel('RMSE (Root Mean Squared Error)', fontweight='bold')
    ax2.set_title('RMSE Comparison\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax2.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax2.text(row['rmse'] + 0.1, i, f"{row['rmse']:.2f}", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 3.MAPE Comparison (Bar)
    # ========================================================================
    ax3 = fig.add_subplot(gs[0, 2])
    comparison_df_sorted = comparison_df.sort_values('mape')
    bars = ax3.barh(comparison_df_sorted['model'], comparison_df_sorted['mape'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax3.set_xlabel('MAPE (%)', fontweight='bold')
    ax3.set_title('MAPE Comparison\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax3.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax3.text(row['mape'] + 1, i, f"{row['mape']:.1f}%", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 4.Coverage Comparison (Bar with target line)
    # ========================================================================
    ax4 = fig.add_subplot(gs[1, 0])
    comparison_df_sorted = comparison_df.sort_values('coverage', ascending=False)
    bars = ax4.barh(comparison_df_sorted['model'], comparison_df_sorted['coverage'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax4.axvline(x=95, color='red', linestyle='--', linewidth=2, label='Target (95%)', alpha=0.7)
    ax4.set_xlabel('Coverage (%)', fontweight='bold')
    ax4.set_title('Coverage Comparison\n(Target:95%)', fontweight='bold', fontsize=12)
    ax4.legend(loc='lower right')
    ax4.grid(axis='x', alpha=0.3)
    ax4.set_xlim(85, 100)
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax4.text(row['coverage'] + 0.3, i, f"{row['coverage']:.1f}%", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 5.Interval Width Comparison (Bar)
    # ========================================================================
    ax5 = fig.add_subplot(gs[1, 1])
    comparison_df_sorted = comparison_df.sort_values('avg_interval_width')
    bars = ax5.barh(comparison_df_sorted['model'], comparison_df_sorted['avg_interval_width'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax5.set_xlabel('Average Interval Width', fontweight='bold')
    ax5.set_title('Prediction Interval Width\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax5.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax5.text(row['avg_interval_width'] + 0.5, i, f"{row['avg_interval_width']:.1f}", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 6.Prediction Time Comparison (Bar)
    # ========================================================================
    ax6 = fig.add_subplot(gs[1, 2])
    comparison_df_sorted = comparison_df.sort_values('prediction_time')
    bars = ax6.barh(comparison_df_sorted['model'], comparison_df_sorted['prediction_time'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax6.set_xlabel('Prediction Time (seconds)', fontweight='bold')
    ax6.set_title('Computational Efficiency\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax6.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        ax6.text(row['prediction_time'] + 0.001, i, f"{row['prediction_time']:.3f}s", 
                va='center', fontweight='bold', fontsize=9)
    
    # ========================================================================
    # 7.Overall Ranking (Bar)
    # ========================================================================
    ax7 = fig.add_subplot(gs[2, 0])
    comparison_df_sorted = comparison_df.sort_values('overall_rank')
    bars = ax7.barh(comparison_df_sorted['model'], comparison_df_sorted['overall_rank'],
                    color=[model_colors[m] for m in comparison_df_sorted['model']])
    ax7.set_xlabel('Overall Rank Score', fontweight='bold')
    ax7.set_title('üèÜ Overall Ranking\n(Lower is Better)', fontweight='bold', fontsize=12)
    ax7.grid(axis='x', alpha=0.3)
    ax7.invert_xaxis()  # Best (lowest) on top
    
    for i, (idx, row) in enumerate(comparison_df_sorted.iterrows()):
        rank_pos = len(comparison_df_sorted) - i
        ax7.text(row['overall_rank'] - 0.1, i, f"#{rank_pos}", 
                va='center', ha='right', fontweight='bold', fontsize=10,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))
    
    # ========================================================================
    # 8.Radar Chart - Multi-metric comparison
    # ========================================================================
    ax8 = fig.add_subplot(gs[2, 1], projection='polar')
    
    # Normalize metrics to 0-1 scale (invert where necessary)
    metrics_to_plot = ['mae', 'rmse', 'coverage', 'avg_interval_width']
    metrics_labels = ['MAE', 'RMSE', 'Coverage', 'Interval\nWidth']
    
    angles = np.linspace(0, 2 * np.pi, len(metrics_to_plot), endpoint=False).tolist()
    angles += angles[:1]  # Complete the circle
    
    for idx, row in comparison_df.iterrows():
        values = []
        for metric in metrics_to_plot:
            if metric == 'coverage':
                # Coverage: closer to 95% is better
                val = 1 - abs(row[metric] - 95) / 15  # Normalize
            elif metric in ['mae', 'rmse', 'avg_interval_width']:
                # Lower is better - invert and normalize
                max_val = comparison_df[metric].max()
                min_val = comparison_df[metric].min()
                val = 1 - (row[metric] - min_val) / (max_val - min_val) if max_val > min_val else 0.5
            else:
                val = row[metric]
            values.append(max(0, min(1, val)))  # Clamp to [0, 1]
        
        values += values[:1]  # Complete the circle
        
        ax8.plot(angles, values, 'o-', linewidth=2, label=row['model'], 
                color=model_colors[row['model']], alpha=0.7)
        ax8.fill(angles, values, alpha=0.15, color=model_colors[row['model']])
    
    ax8.set_xticks(angles[:-1])
    ax8.set_xticklabels(metrics_labels, fontsize=9)
    ax8.set_ylim(0, 1)
    ax8.set_title('Multi-Metric Performance\n(Normalized)', 
                  fontweight='bold', fontsize=12, pad=20)
    ax8.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=8)
    ax8.grid(True, alpha=0.3)
    
    # ========================================================================
    # 9.Metrics Summary Table
    # ========================================================================
    ax9 = fig.add_subplot(gs[2, 2])
    ax9.axis('tight')
    ax9.axis('off')
    
    # Prepare table data
    table_data = []
    for idx, row in comparison_df.sort_values('overall_rank').iterrows():
        rank = list(comparison_df.sort_values('overall_rank')['model']).index(row['model']) + 1
        table_data.append([
            f"#{rank}",
            row['model'],
            f"{row['mae']:.2f}",
            f"{row['coverage']:.1f}%",
            f"{row['avg_interval_width']:.1f}"
        ])
    
    table = ax9.table(cellText=table_data,
                     colLabels=['Rank', 'Model', 'MAE', 'Coverage', 'Width'],
                     cellLoc='center',
                     loc='center',
                     colWidths=[0.12, 0.35, 0.18, 0.18, 0.17])
    
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)
    
    # Style header
    for i in range(5):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # Style rows - highlight best model
    for i in range(1, len(table_data) + 1):
        if i == 1: # Best model
            for j in range(5):
                table[(i, j)].set_facecolor('#FFF9C4')
                table[(i, j)].set_text_props(weight='bold')
    
    ax9.set_title('Performance Summary', fontweight='bold', fontsize=12, pad=10)
    
    # ========================================================================
    # Main Title
    # ========================================================================
    fig.suptitle('üìä Multi-Model Forecasting Comparison\nConformal Prediction + CVaR Optimization', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # Save
    output_path = output_dir / 'model_comparison_comprehensive.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    logger.info(f"Comprehensive comparison saved to {output_path}")
    plt.close()
    
    # ========================================================================
    # Additional Figure:Accuracy vs Coverage Trade-off
    # ========================================================================
    fig, ax = plt.subplots(figsize=(10, 6))
    
    for idx, row in comparison_df.iterrows():
        ax.scatter(row['coverage'], row['mae'], 
                  s=500, alpha=0.6, 
                  color=model_colors[row['model']],
                  edgecolors='black', linewidth=2,
                  label=row['model'])
        
        # Add model name
        ax.annotate(row['model'], 
                   (row['coverage'], row['mae']),
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=10, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7))
    
    # Target lines
    ax.axvline(x=95, color='red', linestyle='--', linewidth=2, label='Target Coverage (95%)', alpha=0.5)
    ax.axhline(y=comparison_df['mae'].min(), color='green', linestyle='--', 
              linewidth=2, label='Best MAE', alpha=0.5)
    
    ax.set_xlabel('Coverage (%)', fontsize=12, fontweight='bold')
    ax.set_ylabel('MAE (Mean Absolute Error)', fontsize=12, fontweight='bold')
    ax.set_title('Accuracy vs Coverage Trade-off\n(Ideal:Top-Right Corner)', 
                fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend(loc='best', fontsize=9)
    
    # Highlight optimal region
    ax.axvspan(95, 100, alpha=0.1, color='green', label='Optimal Coverage')
    
    output_path = output_dir / 'accuracy_vs_coverage.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    logger.info(f"Accuracy vs Coverage plot saved to {output_path}")
    plt.close()
    
    # ========================================================================
    # Additional Figure:Pareto Front (MAE vs Interval Width)
    # ========================================================================
    fig, ax = plt.subplots(figsize=(10, 6))
    
    for idx, row in comparison_df.iterrows():
        ax.scatter(row['avg_interval_width'], row['mae'], 
                  s=500, alpha=0.6,
                  color=model_colors[row['model']],
                  edgecolors='black', linewidth=2)
        
        ax.annotate(row['model'], 
                   (row['avg_interval_width'], row['mae']),
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=10, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7))
    
    ax.set_xlabel('Average Interval Width', fontsize=12, fontweight='bold')
    ax.set_ylabel('MAE (Mean Absolute Error)', fontsize=12, fontweight='bold')
    ax.set_title('Pareto Front: Accuracy vs Precision\n(Ideal:Bottom-Left Corner)', 
                fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # Add optimal region shading
    best_mae = comparison_df['mae'].min()
    best_width = comparison_df['avg_interval_width'].min()
    ax.axhline(y=best_mae, color='green', linestyle='--', alpha=0.3, label='Best MAE')
    ax.axvline(x=best_width, color='blue', linestyle='--', alpha=0.3, label='Best Width')
    ax.legend(loc='best')
    
    output_path = output_dir / 'pareto_mae_vs_width.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    logger.info(f"Pareto front plot saved to {output_path}")
    plt.close()
    
    logger.info(f"‚úÖ All model comparison visualizations generated in {output_dir}")


def plot_forecast_intervals(
    y_test:pd.Series,
    predictions:pd.DataFrame,
    df_test:pd.DataFrame,
    output_dir:str = 'outputs/figures'
):
    """
    Plota s√©ries temporais com intervalos de previs√£o.
    
    Args:
        y_test:Valores reais
        predictions:Previs√µes com intervalos
        df_test:DataFrame original de teste com datas
        output_dir:Diret√≥rio para salvar figuras
    """
    logger.info("Gerando gr√°ficos de intervalos de previs√£o")
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Preparar dados
    df_plot = df_test.copy().reset_index(drop=True)
    df_plot['actual'] = y_test.values
    df_plot['lower'] = predictions['lower'].values
    df_plot['point'] = predictions['point'].values
    df_plot['upper'] = predictions['upper'].values
    
    # Plot por produto
    products = df_plot['product'].unique()
    n_products = len(products)
    
    fig, axes = plt.subplots(3, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for idx, product in enumerate(products):
        ax = axes[idx]
        df_prod = df_plot[df_plot['product'] == product].sort_values('date')
        
        ax.plot(df_prod['date'], df_prod['actual'], 'o-', label='Real', color='black', markersize=4)
        ax.plot(df_prod['date'], df_prod['point'], '--', label='Previs√£o', color='blue', linewidth=2)
        ax.fill_between(
            df_prod['date'],
            df_prod['lower'],
            df_prod['upper'],
            alpha=0.3,
            label='Intervalo 90%',
            color='skyblue'
        )
        
        ax.set_title(f'{product}', fontsize=10, fontweight='bold')
        ax.set_xlabel('Data', fontsize=8)
        ax.set_ylabel('Quantidade', fontsize=8)
        ax.legend(fontsize=7)
        ax.tick_params(axis='x', rotation=45, labelsize=7)
        ax.tick_params(axis='y', labelsize=7)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    filepath = Path(output_dir) / 'forecast_intervals.png'
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    logger.info(f"Gr√°fico salvo em {filepath}")
    plt.close()


def plot_coverage_analysis(
    y_test:pd.Series,
    predictions:pd.DataFrame,
    df_test:pd.DataFrame,
    output_dir:str = 'outputs/figures'
):
    """
    Analisa coverage dos intervalos por produto.
    
    Args:
        y_test:Valores reais
        predictions:Previs√µes com intervalos
        df_test:DataFrame de teste
        output_dir: Diret√≥rio para salvar
    """
    logger.info("Gerando an√°lise de coverage")
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    df_analysis = df_test.copy().reset_index(drop=True)
    df_analysis['actual'] = y_test.values
    df_analysis['lower'] = predictions['lower'].values
    df_analysis['upper'] = predictions['upper'].values
    df_analysis['in_interval'] = (
        (df_analysis['actual'] >= df_analysis['lower']) &
        (df_analysis['actual'] <= df_analysis['upper'])
    )
    
    # Coverage por produto
    coverage_by_product = df_analysis.groupby('product')['in_interval'].mean() * 100
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Gr√°fico 1:Coverage por produto
    ax1 = axes[0]
    coverage_by_product.plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black')
    ax1.axhline(y=90, color='red', linestyle='--', linewidth=2, label='Target 90%')
    ax1.set_title('Coverage por Produto', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Produto', fontsize=10)
    ax1.set_ylabel('Coverage (%)', fontsize=10)
    ax1.legend()
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Gr√°fico 2:Largura do intervalo
    df_analysis['interval_width'] = df_analysis['upper'] - df_analysis['lower']
    width_by_product = df_analysis.groupby('product')['interval_width'].mean()
    
    ax2 = axes[1]
    width_by_product.plot(kind='bar', ax=ax2, color='coral', edgecolor='black')
    ax2.set_title('Largura M√©dia do Intervalo por Produto', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Produto', fontsize=10)
    ax2.set_ylabel('Largura M√©dia', fontsize=10)
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    filepath = Path(output_dir) / 'coverage_analysis.png'
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    logger.info(f"Gr√°fico salvo em {filepath}")
    plt.close()


def plot_decision_comparison(
    metrics_decision:Dict,
    output_dir:str = 'outputs/figures'
):
    """
    Compara custos entre CVaR e baselines.
    
    Args:
        metrics_decision:M√©tricas de decis√£o
        output_dir:Diret√≥rio para salvar
    """
    logger.info("Gerando compara√ß√£o de decis√µes")
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # FIX:Usar nomes corretos das chaves
    methods = ['CVaR', 'Mean', 'Newsvendor', 'Quantile']
    costs = [
        metrics_decision['CVaR Cost'],  # Removido (avg)
        metrics_decision['Mean Baseline Cost'],
        metrics_decision['Newsvendor Cost'],
        metrics_decision['Quantile Cost']
    ]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.bar(methods, costs, edgecolor='black', linewidth=1.5)
    
    # Colorir CVaR de verde se for melhor que pelo menos um baseline
    if costs[0] < costs[2]: # CVaR vs Newsvendor
        bars[0].set_color('darkgreen')
    else:
        bars[0].set_color('orange')
    
    for i in range(1, 4):
        bars[i].set_color('gray')
    
    ax.set_title('Compara√ß√£o de Custos: CVaR vs Baselines', fontsize=14, fontweight='bold')
    ax.set_xlabel('M√©todo', fontsize=12)
    ax.set_ylabel('Custo Total do Per√≠odo', fontsize=12)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bar, cost in zip(bars, costs):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'${cost:.0f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # Adicionar linha de economia
    ax.axhline(y=costs[0], color='green', linestyle='--', linewidth=1, alpha=0.5)
    
    plt.tight_layout()
    filepath = Path(output_dir) / 'decision_comparison.png'
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    logger.info(f"Gr√°fico salvo em {filepath}")
    plt.close()


def generate_report(
    metrics_forecast:Dict,
    metrics_decision:Dict,
    optimization_results:Dict,
    output_dir:str = 'outputs'
):
    """
    Gera relat√≥rio em Markdown com todos os resultados.
    
    Args:
        metrics_forecast: M√©tricas de previs√£o
        metrics_decision:M√©tricas de decis√£o
        optimization_results:Resultados da otimiza√ß√£o
        output_dir: Diret√≥rio para salvar
    """
    logger.info("Gerando relat√≥rio")
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Formatar aloca√ß√µes
    allocations_text = ""
    product_names = [
        'Camarao 36/40 120G', 'Camarao 36/40 200G', 'FILE DE TILAPIA 170G',
        'File de Robalo 170G', 'File de robalo (moqueca) 300G', 'Fileto 100G',
        'Parmegiana 150G', 'TILAPIA 300G', 'Tournedour 150G'
    ]
    
    for i, qty in enumerate(optimization_results['q_optimal']):
        allocations_text += f"- {product_names[i]}:{qty:.2f} unidades\n"
    
    report = f"""# Relat√≥rio: Sistema de Aloca√ß√£o √ìtima com Conformal Prediction + CVaR

**Data de Gera√ß√£o**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1.Resumo Executivo

Este relat√≥rio apresenta os resultados do sistema de aloca√ß√£o √≥tima de estoque usando:
- **Conformal Prediction** para intervalos de previs√£o calibrados
- **CVaR Optimization** para decis√µes robustas de aloca√ß√£o

### Principais Resultados

‚úÖ **Service Level**:{metrics_decision['CVaR Service Level (%)']:.1f}% (Target:>90%)  
{'‚úÖ' if metrics_decision['Savings vs Newsvendor (%)'] > 0 else '‚ö†Ô∏è'} **Economia vs Newsvendor**:{metrics_decision['Savings vs Newsvendor (%)']:.1f}%  
‚ö†Ô∏è **Coverage**:{metrics_forecast['Coverage (%)']:.1f}% (Target:90%)

---

## 2.M√©tricas de Previs√£o

### 2.1 Acur√°cia Pontual
- **MAE (Mean Absolute Error)**:{metrics_forecast['MAE']:.2f}
- **RMSE (Root Mean Squared Error)**:{metrics_forecast['RMSE']:.2f}
- **MAPE (Mean Absolute Percentage Error)**:{metrics_forecast['MAPE']:.2f}%

### 2.2 Qualidade dos Intervalos
- **Coverage Real**:{metrics_forecast['Coverage (%)']:.2f}% (Target:90%)
- **Largura M√©dia do Intervalo**:{metrics_forecast['Avg Interval Width']:.2f}

{'‚úÖ' if metrics_forecast['Coverage (%)'] >= 85 else '‚ö†Ô∏è'} **Status Coverage**:{'APROVADO' if metrics_forecast['Coverage (%)'] >= 85 else 'NECESSITA AJUSTE'}

---

## 3.M√©tricas de Decis√£o

### 3.1 Custos Operacionais (Per√≠odo Total de Teste)
- **CVaR**:${metrics_decision['CVaR Cost']:.2f}
- **Mean Baseline**:${metrics_decision['Mean Baseline Cost']:.2f}
- **Newsvendor**:${metrics_decision['Newsvendor Cost']:.2f}
- **Quantile**:${metrics_decision['Quantile Cost']:.2f}

### 3.2 Economia Gerada
- **vs Mean Baseline**:{metrics_decision['Savings vs Mean (%)']:.2f}%
- **vs Newsvendor**:{metrics_decision['Savings vs Newsvendor (%)']:.2f}%

### 3.3 N√≠vel de Servi√ßo
- **Service Level**: {metrics_decision['CVaR Service Level (%)']:.2f}% (Target:>90%)

{'‚úÖ' if metrics_decision['Savings vs Newsvendor (%)'] > 0 else '‚ö†Ô∏è'} **Status Economia**:{'APROVADO' if metrics_decision['Savings vs Newsvendor (%)'] > 10 else 'PARCIAL'}  
{'‚úÖ' if metrics_decision['CVaR Service Level (%)'] >= 90 else '‚ö†Ô∏è'} **Status Service Level**:{'APROVADO' if metrics_decision['CVaR Service Level (%)'] >= 90 else 'ATEN√á√ÉO'}

---

## 4.Otimiza√ß√£o CVaR

### 4.1 Resultados
- **CVaR (10% piores cen√°rios)**:${optimization_results['cvar']:.2f}
- **Custo Esperado**: ${optimization_results['expected_cost']:.2f}
- **Pior Caso**:${optimization_results['worst_case_cost']:.2f}

### 4.2 Aloca√ß√£o √ìtima por Produto
{allocations_text}

---

## 5.Visualiza√ß√µes

![Intervalos de Previs√£o](figures/forecast_intervals.png)

![An√°lise de Coverage](figures/coverage_analysis.png)

![Compara√ß√£o de Decis√µes](figures/decision_comparison.png)

---

## 6.Conclus√µes e Recomenda√ß√µes

### 6.1 Principais Insights

1.**CVaR vs Newsvendor**:O CVaR obteve {metrics_decision['Savings vs Newsvendor (%)']:.1f}% de economia comparado ao Newsvendor, demonstrando robustez
2.**Service Level Excelente**:{metrics_decision['CVaR Service Level (%)']:.1f}% indica alta capacidade de atendimento da demanda
3.**Mean Baseline Otimista**:A baseline Mean apresenta custo muito baixo, sugerindo subestima√ß√£o da demanda real
4.**Coverage Abaixo do Target**:{metrics_forecast['Coverage (%)']:.1f}% sugere necessidade de ajuste nos intervalos

### 6.2 Recomenda√ß√µes

1.**Ajustar Alpha Conformal**:Reduzir alpha de 0.1 para 0.05 para melhorar coverage
2.**Validar Custos**:Verificar se custos de underage (${10.0}) e overage (${3.0}) refletem realidade operacional
3.**Features Adicionais**:Incluir promo√ß√µes, eventos e sazonalidade espec√≠fica
4.**Monitoramento Cont√≠nuo**:Implementar tracking de cobertura em produ√ß√£o

### 6.3 Interpreta√ß√£o dos Resultados

O CVaR demonstra comportamento **conservador e robusto**:
- Aloca mais estoque que a previs√£o pontual (Mean)
- Garante 100% de service level
- Minimiza risco nos piores cen√°rios
- Economia positiva vs Newsvendor tradicional

---

## 7.Crit√©rios de Sucesso

| Crit√©rio | Target | Resultado | Status |
|----------|--------|-----------|--------|
| Coverage | ‚â•85% | {metrics_forecast['Coverage (%)']:.1f}% | {'‚úÖ' if metrics_forecast['Coverage (%)'] >= 85 else '‚ö†Ô∏è'} |
| Economia vs Newsvendor | ‚â•10% | {metrics_decision['Savings vs Newsvendor (%)']:.1f}% | {'‚úÖ' if metrics_decision['Savings vs Newsvendor (%)'] >= 10 else '‚ö†Ô∏è'} |
| Service Level | >90% | {metrics_decision['CVaR Service Level (%)']:.1f}% | {'‚úÖ' if metrics_decision['CVaR Service Level (%)'] >= 90 else '‚ö†Ô∏è'} |

---

**Fim do Relat√≥rio**
"""
    
    filepath = Path(output_dir) / 'report.md'
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"Relat√≥rio salvo em {filepath}")